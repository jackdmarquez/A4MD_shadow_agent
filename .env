# =========================
# Shadow / Provenance Q&A
# =========================

# auto = intenta OpenAI si OPENAI_API_KEY existe; si no, intenta Ollama; si no, cae a modo sin LLM
# openai | ollama | none | auto
SHADOW_LLM_PROVIDER=auto

# ---- OpenAI (opcional) ----
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.2

# ---- Ollama local (opcional) ----
# Ollama por defecto sirve en http://localhost:11434
OLLAMA_BASE_URL=http://10.0.0.109:11434
# Ejemplos: llama3.1, mistral, qwen2.5, etc. (tiene que estar "pulled" localmente)
OLLAMA_MODEL=llama3.1:latest
OLLAMA_TEMPERATURE=0.2
